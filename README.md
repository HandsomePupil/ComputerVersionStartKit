# ComputerVersionStartKit
Learning CV with Zero-IQ ?

Fine,"Watermelon Book or Flower Book"  are't fit to idiot like me !

So build a StartKit(what is really fundational knowledge)for myself !

Next CV Knowledge-Trees are copyed from someone's paper,read it may help you build the knowledge structure of CV fastly and briefly.

```
Thanks to the original author Wang Xiaotian. Currently working at BAT,  he is a senior technical 
expert in AI algorithm.He graduated from top 3 University in France with a double master's degree
(double master's degree in computer science and mathematical application).
```

[写在一切的前面：一本顶级的电子图书](https://luweikxy.gitbook.io/machine-learning-notes/)

[写在一切的前面：北大R语言学习电子书](https://www.math.pku.edu.cn/teachers/lidf/docs/Rbook/html/_Rbook/index.html)

[写在一切的前面：R绘图](https://r-graph-gallery.com/)

**第一章：机器学习与计算机视觉**
***

**计算机视觉简介**

_技术背景_

`了解人工智能方向、热点`

_计算机视觉简介_

`cv简介`<br>
`cv技能树构建`<br>
`应用领域 `<br>

_机器学习的数学基础_

`线性与非线性变换`<br>
`概率学基础`<br>
`熵`<br>
`kl散度`<br>
`梯度下降法`<br>

 **计算机视觉与机器学习基础**
 
_图像和视频_  

>传统的CV与[数字图像处理](https://blog.csdn.net/weixin_43848614/article/details/122255196)息息相关，以下是两篇能快速了解一些热点专业词汇的文章  
>[数字图像处理基础1](https://blog.csdn.net/Strive_0902/article/details/78026816)  
>[数字图像处理基础2](https://blog.csdn.net/qq_39431829/article/details/103350365)

`图像的取样与量化`<br>
`滤波`<br>
`直方图`<br>
`上采样`<br>
`下采样`<br>
`卷积`<br>
`直方图均衡化算法`<br>
`最近邻差值`<br>
`单/双线性差值 `<br>

_特征选择与特征提取_

`特征选择方法`<br>
`filter等`<br>
`特征提取方法：PCA、LDA、SVD等 `<br>

_边缘提取_

`Canny`<br>
`Roberts`<br>
`Sobel`<br>
`Prewitt`<br>
`Hessian特征`<br>
`Haar特征 `<br>

_相机模型_

`小孔成像模型`<br>
`相机模型`<br>
`镜头畸变`<br>
`透视变换 `<br>

**计算机视觉与机器学习进阶**

_聚类算法_

`kmeans`<br>
`层次聚类`<br>
`密度聚类`<br>
`谱聚类 `<br>

_图像滤波器_

`直通滤波`<br>
`体素滤波`<br>
`双边滤波器`<br>
`条件滤波`<br>
`半径滤波`<br>
`图像增加噪声与降噪`<br>

_数学方法_

[`蒙特卡洛法`](https://zhuanlan.zhihu.com/p/150729238)<br>
 **OpenCV详解**
 
_OpenCV算法解析_

`线性拟合`<br>
`最小二乘法`<br>
`RANSAC算法`<br>
`哈希算法`<br>
`DCT算法`<br>
`汉明距离`<br>
`图像相似度 `<br>

**第二章：深度学习与计算机视觉**
***
**神经网络**

_深度学习与神经网络_

`深度学习简介`<br>
`基本的深度学习架构`<br>
`神经元`<br>
`激活函数详解（sigmoid、softmax、tanh、relu等）`<br>
>[sigmoid详解---](https://zhuanlan.zhihu.com/p/24990626)
>[softmax详解---](https://blog.csdn.net/lz_peter/article/details/84574716)
>[relu详解---](https://www.jianshu.com/p/338afb1389c9)

`感性认识隐藏层`<br>
`如何定义网络层`<br>
`损失函数` <br>
>[CE，MSE，交叉熵损失函数---](https://zhuanlan.zhihu.com/p/35709485)

_推理和训练_

`神经网络的推理和训练`<br>
>[实战：在../Codingry/CNN文件夹中持续更新几大经典网络的推理和训练代码](https://github.com/HandsomePupil/ComputerVersionStartKit/tree/main/CodingTry/CNN)

`bp算法详解`  
`归一化`<br>
`Normalization详解`  
>[Batch Normalization](https://blog.csdn.net/qq_37541097/article/details/104434557)  
>[Layer Normalization](https://blog.csdn.net/qq_37541097/article/details/117653177)  
>[Group Normalization](https://blog.csdn.net/qq_37541097/article/details/118016048)  

`解决过拟合`  
>[ExponentialLR可变学习率](https://zhuanlan.zhihu.com/p/336673463)

`drop方法`  
>[dropout(0.5)?Rescale如何弥补dropout的参数？](https://zhuanlan.zhihu.com/p/77609689)  
>[droppath(0.5)?droppath理解](https://liubingqing.blog.csdn.net/article/details/122887143)

`softmax`<br>
`手推神经网络的训练过程 `<br>

_从零开始训练神经网络_

`使用python从零开始实现神经网络训练`<br>
`构建神经网络的经验总结 `<br>

_深度学习开源框架_

`pytorch`<br>
`tensorflow`<br>
`caffe`<br>
`mxnet`<br>
`keras`<br>
`优化器详解(GD,SGD,RMSprop等`<br>



**不错的知识点解析**
***
[细讲Attention](https://cloud.tencent.com/developer/article/1377062)

[何为Ground-truth,框是怎么预测的](https://www.cnblogs.com/boligongzhu/p/15066380.html)

[GIoU](https://blog.csdn.net/weixin_41735859/article/details/89288493)

**入门论文**
***
[《Gradient-based learning applied to document recognition》CNN万物之源](https://ieeexplore.ieee.org/document/726791)

[《ImageNet Classification with Deep Convolutional Neural Networks》2012冠军网络AlexNet](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)

[《Going Deeper with Convolutions》2015CVPR GoogLeNet 作者大写了L用于致敬LeNet！](https://research.google/pubs/pub43022.pdf)

[《Deep Residual Learning for Image Recognition》工业界滴xing！](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)
>续文《ResNeXt》中提到了“组卷积-Group Convolutional”，在大于3层的模块中有一定效果。

**CNN相关**
***
[Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs大核卷积](https://arxiv.org/pdf/2203.06717.pdf )

**Transformer相关**
***
[Attention Is All You Need 2017开山之作](https://arxiv.org/pdf/1706.03762.pdf)

[Attention的整体概述](https://zhuanlan.zhihu.com/p/37601161)

[NLP中的Attention原理和源码解析](https://zhuanlan.zhihu.com/p/43493999)

[Transformer模型原理详解](https://zhuanlan.zhihu.com/p/44121378)

[ViT Transformer应用到CV的首作](https://arxiv.org/pdf/2010.11929.pdf)

[Swin Transformer节约注意力计算量，增加Token间信息交互](https://arxiv.org/pdf/2103.14030.pdf)

[S-Transformer超详细的介绍视频](https://www.bilibili.com/video/BV1pL4y1v7jC/?spm_id_from=333.788)

**传统目标跟踪**
***
[LK光流法](https://www.cnblogs.com/riddick/p/10586662.html)

[粒子滤波跟踪算法图解](https://blog.csdn.net/dieju8330/article/details/85020056)

[MeanShift均值滤波跟踪](https://www.cnblogs.com/wenyangyang/p/15658853.html)

**目标跟踪**
***
[单目标跟踪基本概念与术语](https://zhuanlan.zhihu.com/p/402380960)

[SiamFC 孪生网络跟踪开山之作](https://arxiv.org/pdf/1606.09549)
>[代码细节解读](https://blog.csdn.net/qq_41831753/article/details/113783627)

[MixFormer-跟踪端到端新范式，合并特征提取与融合](https://arxiv.org/pdf/2203.11082)  


**对抗样本**
***
[Intriguing properties of neural networks  源头](https://arxiv.org/pdf/1312.6199.pdf)

[对抗样本领域当前最不错的综述](https://arxiv.org/pdf/1801.00553.pdf)
